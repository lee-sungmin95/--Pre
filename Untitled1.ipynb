{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHaBUES0X/VMWXlV3EmDWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee-sungmin95/--Pre/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs6hMc_aT7cC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDUL3OfkBDaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmcX3PHVFe_Y",
        "colab_type": "text"
      },
      "source": [
        "#언어분야\n",
        ">BERT는 Bidirectional Encoder Represetations from Transformers의 약자로 18년 10월에 구글이 논문으로 발표한 언어모델로 NLP(자연어처리) 사전 훈련 기술이며, 특정 분야에 국한된 기술이 아니라 모든 자연어 처리 분야에서 좋은 성능을 내는 범용 언어모델입니다. 11개 이상의 자연어처리 과제에서 BERT가 지금껏 state-of-the-art 였던 앙상블모델을 가볍게 누르며 최고 성능을 기록하였습니다. 이 모델은 과연 뭐길래 사람의 성능을 능가하는지 알아보겠습니다.\n",
        "\n",
        "\n",
        "구글은 어텐션(Attention) 기반의 언어모델이다. 어텐션의 원리는 말 그대로 입력 정보 중 중요한 단어에 집중해 사용자의 발언 의도를 분석하는 방식이다. 중요한 단어들을 계속해서 업데이트 하며 학습하고 사용자의 발화의도와 문맥 분석에 집중하며 학습을 거듭한다. 구글이 이 어텐션을 바탕으로 2017년에 내놓은 딥러닝 아키텍처가 바로 ‘트랜스포머’다.\n",
        "\n",
        "구글은 트랜스포커 아키텍처를 양방향으로 활용해 BERT를 설계했는데 이 방식이 대화형 AI 개발에 있어서 굉장히 효율적인 방식이다. 기존 RNN을 대체하고 훨씬 더 능률적으로 언어능력을 고도화 할 수 있게 됐다.\n",
        "***\n",
        "---\n",
        "\n",
        "1. Input Representaion    \n",
        ">**BERT는 3가지 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성된다.**\n",
        "\n",
        ">1.1. Token Embeddings  \n",
        "BERT는 기존 흔히 사용되는 워드 임베딩 방식의 임베딩을 사용하지 않았다. *Word Piece 임베딩 방식을 사용하였는데 이 방식은 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 만든다.* 즉, 자주 등장하는 sub-word는 그 자체가 단위가 되고, 자주 등장하지 않는 단어는 sub-word로 쪼개지게 된다. 기존 워드 임베딩 방법은 Out-of-vocabulary(OOV) 의 문제가 있었다. 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있었다. *하지만 Word Piece 임베딩은 모든 언어에 적용 가능하며 sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도 상승효과도 있다.*\n",
        "\n",
        ">1.2 Sentence Embeddings    \n",
        "토큰 시킨 단어들을 다시 하나의 문장으로 만드는 작업입니다. BERT는 두 개의 문장을 문장 구분자([SEP])와 함께 합쳐 넣는다. 입력 길이의 제한으로 두 문장은 합쳐서 512 subword 이하로 제한한다. 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문에 적절한 입력 길이를 설정해야 한다. *한국어는 보통 평균 20subword로 구성되고 99%가 60subword를 넘지 않기 때문에 입력길이를 두 문장이 합쳐 128로 제한하고 학습한 후, 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용한다.*\n",
        "\n",
        ">1.3 Position Embedding    \n",
        "BERT는 저자의 이전 논문인 Trasnformer 모델을 차용하였다. Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용한다. *Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보를 줘야한다. 그래서 Transformer 에서는 Sinusoid 함수를 이용한 Positional encoding을 사용하였고, BERT에서는 이를 변형하여 Position encoding을 사용한다. Position encoding은 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩 한다.*\n",
        "\n",
        ">1.4 임베딩 취합    \n",
        "BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 하나의 임베딩 값으로 만든다. 그리고 이 합에 Layer Normalization과 Dropout을 적용하여 입력으로 사용한다.\n",
        "\n",
        "2. 언어 모델링 구조(pre-training BERT)\n",
        ">**BERT를 이용한 자연어 처리는 2단계로 나눈다.** 거대 Encoder가 입력 문장들을 임베딩 하여 언어를 모델링하는 언어 모델링 구조과정과 이를 fine-tuning하여 자연어 처리 Task를 수행하는 과정이다. 본 절에서는 언어 모델링 구조 과정에 대해 다룬다.    \n",
        "기존의 방법들은 보통 좌-우(left-to-right)로 학습하거나 우-좌(right-to-left)로 학습하는 방식이다. 이 방식들은 입력의 다음 단어를 예측하는데 좋은 성능을 발휘한다. 하지만 BERT는 이 방식들과는 다르게 언어의 특성을 잘 학습하도록 2가지 방식, MLM(Masked Language Model)과 NSP(Next Sentence Prediction)을 사용한다. 기존 방식인 좌-우 언어모델과 비교하여 MLM 방식이 훨씬 좋은 성능을 기록하였다.\n",
        "\n",
        ">2.1 언어 모델링 데이터    \n",
        "BERT는 총 3.3억 단어(8억 단어의 BookCorpus 데이터와 25억 단어의 Wikipedia 데이터)의 거대한 말뭉치를 이용하여 학습한다. 거대한 말뭉치를 MLM, NSP 모델 적용을 위해 스스로 라벨을 만들고 수행하므로 준지도(Semi-supervised)학습이라고 한다. Wikipedia와 BookCorpus를 정제하기 위해 list, table, header를 제거하였다. 그리고 문장의 순서를 고려해야 하므로 문단 단위로 분리하였고 많은 데이터 정제 작업을 수행하였다.\n",
        "\n",
        ">2.2 모델 구조    \n",
        "BERT모델은 Transformer를 기반으로 한다. Transformer 모델 구조는 인코더-디코더 모델이며 번역 도메인에서 최고성능을 기록하였다. 기존 인코더-디코더 모델들과 다르게 Transformer는 CNN, RNN을 이용하지 않고 Self-attention 이라는 개념을 도입하였다. BERT는 Transformer의 인코더-디코더 중 인코더만 사용하는 모델이다.\n",
        "BERT는 2가지 버전이 있다. BERT-base(L=12, H=768, A=12), BERT-large(L=24, H=1024, A=16)이다. L은 Transformer 블록의 숫자이고 H는 hidden size, A는 Transformer의 Attention block 숫자이다. 즉 L, H, A가 크다는 것은 블록을 많이 쌓았고, 표현하는 은닉층이 크며 Attention 개수를 많이 사용하였다는 뜻이다. BERT-base는 1.1억개의 학습 파라미터, BERT-large는 3.4억개의 학습 파라미터가 있다. BERT는 학습 파라미터가 매우 많으므로 학습시간이 무척 오래 걸린다.\n",
        "\n",
        ">2.3 MLM(Masked Laguage Model)    \n",
        "입력 문장에서 임의로 Token을 마리고(masking), 그 Token을 맞추는 방식인 MLM 학습을 진행한다. 문장의 빈칸 채우기 문제를 학습한다고 생각하면 된다. BERT 이후의 변종들에서 sub-word 단위로 쪼개진 Token을 마스킹 하는게 아니라, 한 단어를 통째로 마스킹 하는 whole word masking 방법을 사용하기도 한다. 한국어는 subword 단위로 쪼개는 방식보다 형태소 단위로 쪼개서 마스킹하는 방식이 더 효과가 좋다고 알려져 있다.    \n",
        "생성모델 계열은(예를 들어 GPT) 입력의 다음 단어를 예측한다. 하지만 MLM은 문장 내 랜덤한 단어를 마스킹하고 이를 예측하는 차이가 있다. 아래 그림과 같이 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹한다. 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둔다. 이는 미세 조정시 올바른 예측을 돕도록 마스킹에 노이즈를 섞는다.    \n",
        "아래는 MLM의 학습과정이다. 입력단어의 15%가 [MASK]가 어떤 단어인지를 예측한다. BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용하고, Word piece의 단어수는 30,522 단어이다. 그러므로 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개 이다.\n",
        "\n",
        ">2.4 NSP(Next Sentnece Prediction)        \n",
        "NSP는 두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식이다. 두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 두 문장이 연관이 있는지를 맞추도록 학습한다. 아래는 NSP의 입력 예시이다. 위에서 설명한 MLM과 동시에 NSP도 적용된 문장들이다. 첫 번째 문장과 두 번째 문장은 [SEP]로 구분한다. 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관없는 문장으로 구성된다. 이 학습을 통해 문맥과 순서를 언어모델이 학습할 수 있다.    \n",
        "아래는 NSP의 학습 방법이다. 연속 문장인지, 아닌지만 판단하면 되므로 Softmax의 출력은 2개이고 3만개의 출력을 갖는 MLM에 비해 빠르게 학습된다.\n",
        "\n",
        "3. 학습된 언어모델 전이학습(Transfer Learning)\n",
        ">미세조정은 2절에서 학습한 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정이다. 실질적으로 성능이 관찰되는 것은 전이학습 이지만, 언어 모델이 제대로 학습되야 전이학습 시 좋은 성능이 나온다. BERT 등장이전에 기존 알고리즘들은 자연어의 다양한 Task에 예를들어 NLI, QA문제를 풀고 싶다하면 각각의 알고리즘을 독립적으로 만들어야 했다. 하지만 BERT 개발 이후 많은 자연어 처리 연구자들은 언어 모델을 만드는데 더 공을 들이게 되었다. 그리고 전이학습의 Task의 성능도 훨씬 좋아졌다. 전이학습은 라벨이 주어지므로 지도학습(Supervised learning)이다.    \n",
        "전이학습은 BERT의 언어 모델의 출력에 추가적인 모델을 쌓아 만든다. 일반적으로 복잡한 CNN, LSTM, Attention을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려져 있다.    \n",
        "BERT를 각 Task에 쓰기위한 예시는 위 그림과 같다. (a)는 문장 쌍 분류 문제로 두 문장을 하나의 입력으로 넣고 두 문장 간 관계를 구한다. (b)는 한 문장을 입력으로 넣고 문장의 종류를 분류하는 문제이다. (c)는 문장이나 문단 내에서 원하는 정답 위치의 시작과 끝을 구한다. (d)는 입력문장 Token들의 개체명(Named Entity Recognition) 을 구하거나 품사(Part-of-speech tagging)를 구하는 문제이다. 다른 Task들과 다르게 입력의 모든 Token들에 대해 결과를 구한다.    \n",
        "위 그림은 일반적인 자연어 Task들을 묶은 GLUE 벤치마크 이다. 8개의 데이터셋의 평균으로 언어모델의 성능을 비교한다. BERT-base는 기존 SOTA 알고리즘들을 크게 앞질렀고 심지어 파라미터 수가 훨씬 많은 BERT-large는 +8% 정도의 성능 향상을 보여준다.\n",
        "\n",
        "4. 비교실험\n",
        "\n",
        ">4.1 학습방법 비교    \n",
        "BERT의 2가지 학습방법(MLM, NSP)을 제거해보며 결과를 비교해보았다. No NSP는 MLM만 적용한 방식, LTR & NO NSP는 MLM 대신 left-to-right 언어모델 방식으로 학습한 방식이다. left-to-right 방식보다 MLM 방식이 엄청난 성능 향상을 가져왔다. NSP 유무도 NLI 계열의 문제에서 성능이 많이 하락하였다. NSP가 문장간의 논리 구조를 학습하는데 중요함을 알 수 있다.\n",
        "\n",
        ">4.2 모델 크기 효과    \n",
        "위의 그림은 모델사이즈와 성능에 대한 결과 비교이다. 확실히 모델이 커질수록(L, H, Z) 큰 성능향상이 있었다. 하지만 TPU를 사용하는 구글이 아닌 일반 기업에서는 모델의 사이즈를 무한정으로 키우는건 불가능하다.\n",
        "\n",
        "5. 결론\n",
        ">18년 10월 BERT가 발표된 이래로 이를 약간씩 변형한 수 많은 모델들이 쏟아져 나오고 있다. 구글, Facebook, MS 등등 엄청난 리소스와 인력으로 일반 기업에서는 시도도 못할 정도이다. 자연어처리 분야가 점점 리소스가 없으면 접근도 할 수 없이 진입장벽이 높아지는 느낌이다. BERT 계열은 훌륭한 성능을 내지만 학습시간이 무척 오래 걸린다. 그리고 BERT는 일반 도메인 데이터로 학습되었기 때문에 일반 자연어처리 문제에서는 무척 잘 작동한다. 하지만 특정 분야(Bio, Science, Finance)에는 잘 동작하지 않는다. 사용 단어들이 다르고 언어의 특성이 다르기 때문이다. 그러므로 특정 분야에서 사용하려면 특정 분야의 학습데이터를 수집하여 언어모델 학습을 추가로 진행해 줘야 한다. 하지만 큰 문제가 있다. 이런 학습 데이터를 만들기 위해 수많은 인력과 노력, 시간이 필요하다. 올 한해간 학습데이터를 수집, 정제하고 BERT의 입력데이터 생성에 몇 달의 시간이 걸렸다. 이뿐만 아니라 언어모델 학습에 훌륭한 서버(Tesla V100 X4, 32G)를 사용하여도 한 달의 학습시간이 소요되었다.    \n",
        "BERT의 성능을 증가히기 위해서 더 큰 모델과 학습 데이터를 이용하는 XLNet, RoBERTa 연구가 진행되었지만 학습시간을 줄이기 위해 Floating point 16을 사용하는 방법이나, 모델을 경량화 하는 ALBERT, Knowledge distillation 연구도 활발하게 진행되고 있다.    \n",
        "빠르게 발전하는 자연어처리 모델들을 따라잡는것도 중요하지만 역시 실제 운용을 위한 모델 경량화, 데이터전처리 작업이 훨씬 더 중요하다.\n",
        "\n",
        ">[참고자료1](https://ebbnflow.tistory.com/151)    \n",
        "[참고자료2](https://keep-steady.tistory.com/19?category=702926)    \n",
        "[참고자료3](https://blogs.nvidia.co.kr/2020/02/20/bert/)    \n",
        "[참고자료4](http://docs.likejazz.com/bert/)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2LQmR4TUnBi",
        "colab_type": "text"
      },
      "source": [
        "#음성분야\n",
        "\n",
        "한정된 자원을 활용한 음성합성\n",
        "음성합성(speech synthesis)은 주어진 텍스트를 정확하고 자연스럽게 읽는 음성 데이터를 생성하는 기술을 의미한다. 주어진 텍스트에 대응하는 음성을 출력한다는 점에서 TTS(text to speech)라고 표현하기도 한다.\n",
        "\n",
        "1990년대 초 규칙 기반 음성합성 기술에 이어, 2000년대에는 대용량 음성 데이터 기반 음편조합(unit selection)방식이 주목을 받았다. 이 방식은 짧은 단위의 음편을 저장한 후 텍스트에 해당하는 음편을 연결해 합성음을 출력한다. 실제 사람의 음성을 녹음한 음편이기에 원음에 가까운 고품질 합성음을 만들 수 있다는 장점이 있는 한편, 음편의 연결 경계가 부자연스럽고 주어진 문장에 항상 똑같이 말하는 문제 등의 단점을 지닌다. 이러한 한계를 극복하고자 통계적 파라미터 방식이 고안됐으며, 이후 은닉 마코브 모델(hidden markov model, HMM)과 심층 신경만(deep neural network, DNN)을 기반으로 한 모델로 발전하였다. 현재 웨이브넷(WaveNet), 타코트론(Tacotron), 딥보이스(Deep Voice) 등에서는 LSTM(long short term memory)을 이용한 seq2seq 네트워크가 주로 활용되고 있다.\n",
        "\n",
        "오늘날의 음성합성 시스템은 크게 3단계를 거친다. 첫 번째 단계에서는 문장 데이터에서 특징 정보를 분석한다. 두 번째 단계에서는 문장 내 특징 정보로부터 음성 파라미터를 추출한다. 마지막으로 세 번째 단계에서는 음성 파라미터에서 음성을 재구성(vocoder)한다. 문자 단위로 입력된 텍스트로부터 음성을 합칠 수 있도록 하나의 모듈에서 이루어진 E2E(end to end) 시스템이라고도 볼 수 있다. 기존 방식과 달리 각 모듈에 대한 전문적인 지식이 필요하지 않아 진입장벽이 낮고, 각 모듈에서 손실(loss)가 누적되는 문제를 해결할 수 있다.\n",
        "\n",
        "현재의 음성합성 기술은 인간과 거의 비슷한 음성을 구사할 수 있는 수준까지 도달~~했따~~했다. 특히 지난 2017년 12월 구글(Google)이 발표한 타코트론2(Tacotron2)는 2018년 10월 기준 현존하는 최고 음질의 합성음을 만들어내고 있다. 하지만 여기에는 선행조건이 따른다. 원하는 수준의 목소리 품질을 얻기 위해서는 잡음(noise)이 없는 깨끗한 음질을 다량 확보해야 한다는 점이다. 그러나 현실에서는 이런 데이터를 쉽게 얻을 수 없고, 얻을 수 있더라도 스튜디오 녹음 및 편집 비용이 많이 든다. 기존에 보유하고 있는 데이터 세트는 크기 자체가 부족해 배우지 않은 발음과 억양이 생길 수 밖에 없다.\n",
        "\n",
        "따라서 어느 정도 잡음이 있거나 적은 분량의 음성 데이터로도 깨끗한 목소리를 생성할 수 있는 강인한(robust) 인공지능을 만드는 연구가 후행되어야 할 것이다. 현재 1분 정도의 샘플 데이터만으로는 고품질의 음성 합성 엔진 개발은 현실적으로 불가능에 가깝다. 다만 해당 목소리를 보유한 사람의 목소리인 척 흉내를 내는 모델 정도는 기대해 볼 수 있다.\n",
        "\n",
        "[첨부자료](https://www.kakaobrain.com/blog/36)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjMZX4QvVgPF",
        "colab_type": "text"
      },
      "source": [
        "#이미지분야\n",
        "\n",
        "인공지능(AI) 기반의 위성∙항공 영상 분석 기업 에스에이아이는 2018년 7월 설립된 신생 스타트업 이지만 위성 활용 분야에서 ‘3년 내 글로벌 톱5’의 야심찬 포부를 밝히고 있다. 항공우주의 광활한 영역에서 국내 기업이 경쟁력을 가지는 대표적인 분야가 인공위성이다. 인공위성 개발 업체 쎄트렉아이의 자회사이면서 독자적인 사업 영역을 개척하는 스타트업으로 위성 활용 분야를 공략하고 있다.\n",
        "\n",
        "국내보다 해외를 타깃으로 인지도를 쌓은 에스아이에가 첫손에 꼽는 경쟁력은 AI기술이다. 인공위성에 AI를 접목해 영상을 분석하면서 기존 위성 영상 분석의 한계점을 극복해 나가고 있다. 대표적으로 인공위성에서 중요한 해상도의 경우 하드웨어의 성능을 끌어올리는 것과 비교하면 소프트웨어적 접근을 통해 비용을 대폭 절감할 수 있다. \n",
        "\n",
        "영상 분석은 AI기술뿐만 아니라 인공위성에 대한 이해도가 높아야 할 수 있는 일이다. 주로 미국∙중국∙유럽이 두각을 나타내는 경진대회에서 에스아이에이는 매년 조금씩 존재감을 보이고 있다. 2017년 초 영국의 국방과학연구소가 주관해 위성 영상으로 지도를 만드는 AI경진 대회 ‘영국 DSTL 위성 이미지 기능 탐지(UK DSTL Satellite Imagery Feature Detection)’에 처음 출전해 단번에 은메달을 수상했다. 특히 지난해 참가한 ‘항공 이미지 물체 감지 콘테스트(Object Detection in Aerial Images Contest)’는 같은 항공 영상 데이터 세트에서 객체를 찾는 도전이었는데 총 60여 팀 가운데 2등을 차지하며 전 세계에 에스에이아이의 이름을 알리는 계기가 됐다.\n",
        "\n",
        "에스아이에이가 단기간에 국제 무대에서 인지도를 올릴 수 있었던 비결로 자율성을 꼽는다. 최종 결정을 각 부서 실무자가 내리고 연구팀∙개발팀∙관리지원팀에서 총 13명의 전문가들이 자체 권한을 가지고 움직이고 있다. \n",
        "\n",
        "위성 영상에 대한 ‘접근성’도 기술력 못지않게 중요하다. 이 분야에 뛰어든 스타트업은 국내외 20여개 업체가 있다. 글로벌 강자는 미국의 디지털 글로브(Maxar), 오비탈 인사이트, 에어버스, 플래닛, 데카르트랩스 등이 있다. 주로 미국 실리콘밸리에서 창업된 민간 스타트업들로 대규모 투자를 받아 규모가 커진 곳들이다. 국내에서 에스아이에이가 후발 주자로 이 시장에 뛰어들었지만 ‘글로벌 톱5’라는 목표를 밝힌 근거는 데이터에 대한 접근 권한에서 강점이 있기 때문이다.\n",
        "\n",
        "많은 위성 영상을 확보하는데는 규제가 있을 뿐만아니라 적지 않은 비용이 들어간다. 영상 한 장에 300만~1000만원으로 AI분석을 위해 수천, 수만 장을 확보하기 위해서는 영상에 접근할 수 있는 권한이 필요하다. 에스아이에이는 쎄트렉아이의 자회사이기 때문에 연구 목적으로는 무상에 가깝게 영상을 확보할 수 있고 해외로 수출한 위성도 수월하게 접근할 수 있다. 또 소프트웨어를 분석하기 위한 하드웨어로는 미국 엔디비아와 파트너십을 맺어 사용하고 있다. 마크 해밀턴 엔디비아 부사장은 ‘엔비디아 미 콘퍼런스 2018’키노트 강연에서 한국의 AI적용 사례로 SK하이닉스∙네이버랩스∙LG CNS와 함께 에스아이에이를 언급한 바 있다.\n",
        "\n",
        "국방과 감시 정찰 분야에 주력\n",
        "에스아이에이의 주 고객은 민간보다 정부 기관이다. B2G(기업과 정부 간 거래) 시장에서 주로 국방과 감시 정찰 분야를 주력으로 삼고 있다. 에스아이에이는 두 개의 소프트웨어를 개발해 판매 중이다. AI분석을 위해 위성 영상을 데이터화하는 소프트웨어 프로그램, 항공 위성에서 개별 객체를 파악하는 소프트웨어 프로그램이다. \n",
        "\n",
        "예를 들어 군사 목적으로 영상을 모니터링할 때 현장에 얼마나 많은 대상물이 있고 기존 영상과의 차이점이 무엇인지 파악하는게 중요하다. 이때 사람의 눈으로 일일이 판독하는 데는 영상 한 장을 분석하는 데 평균 40분이 소요된다. 이것을 기계에 맡기면 3분으로 단축된다. 찾고자 하는 대상물을 일일이 표기해 위협이 될 만한 요소를 파악해 주는 역할을 담당한다. 이와 같은 객체 검출, 변화 탐지, 해상도 개선 등에서 강점을 보인다. 올해에만 약 50억원의 수주를 달성했다. 향후 위성 자체에 AI기술을 탑재하는 진화도 모색 중이다.\n",
        "\n",
        ">[네이버기사](https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=050&aid=0000051294)\n",
        "\n",
        "AI기반 위성⋅항공 영상 분석 전문기업 에스아이에이(SI Analytics, 대표 전태균)는 내달 1일 ‘위성 관측 데이터를 활용한 강수량 산출 경진대회’를 진행한다고 3/26일 밝혔다. 대회는 참가자들이 미항공우주국(NASA)에서 공개한 위성관측 데이터를 활용해 강수량을 산출하고 AI 알고리즘을 개발한다. 데이콘(Dacon)이 성능을 비교해 순위를 결정한다. 전태균 대표는 “NASA에서 수집된 데이터를 통해 인공지능 모델을 직접 만들어 볼 수 있는 굉장히 좋은 기회”라며 “이번 관측값을 바탕으로 인공지능 모델을 도출해 NASA의 강수산추 알고리즘을 뛰어넘는 혁신적인 결과를 기대한다”고 말했다.\n",
        "\n",
        ">[인터넷기사](https://www.hellodd.com/?md=news&mt=view&pid=71468)    \n",
        "[위성관측 데이터 활용 강수량 산출 AI 경진대회](https://dacon.io/competitions/official/235591/overview/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSj800rArY2m",
        "colab_type": "text"
      },
      "source": [
        "#자율주행분야\n",
        "영상인식기술이 자율주행자동차에 적용되기 위해서 중요한 2가지가 있다. 바로 **속도와 정확도**이다. 자율주행자동차에서 영상인식기술이 적용되기 위해서는 우선 실시간**(속도)**을 보장하여야한다. 아무리 정확한 알고리즘이라도 오랜 시간이 걸리는 알고리즘 이라면 자율주행자동차에서 사용할 수 없다. _예를 들어 알고리즘의 수행속도가 1초이고, 60km/h의 속도로 주행을 한다면, 알고리즘의 결과는 1초 전의 정보와 약 16.7m 전의 위치에 대한 정보를 가지고 올 것이다. 이는 자율주행자동차에 탑승한 사람뿐만 아니라 주변까지 위험하게 만드는 일이다._ 두 번째로 중요한 것은 **정확도**이다. 매우 빠른 알고리즘이라도 주변의 사물을 제대로 검출하지 못한다면, 위와 마찬가지로 탑승자와 주변까지 위험하게 만들 것이다.\n",
        "\n",
        ">하지만, 여기서 우리는 딜레마에 빠지게 된다. 정확도와 속도 사이에는 Trade-off가 존재하기 때문이다. 알고리즘의 정확도를 향상시키기 위하여 알고리즘을 복잡하게 만들면 속도가 오래 걸리게 되고, 속도를 향상시키기 위해 알고리즘을 간략화시키면 성능이 떨어지게 된다. x축은 수행시간 y축은 정확도를 나타내는 방법 중 하나인 mAP(mean Average Precision)그래프에서 x축에서 왼쪽으로 갈수록 수행시간이 짧으므로 좋고, 성능은 y축에서 위로 갈수록 좋다. 왼쪽 위에 있을수록 속도가 빠르고 정확도가 높은 알고리즘이다. (그래프에는 RetinaNet-50, RetinaNet-101, Light Head R-CNN, SSD321, DSSD321, R-FCN, SSD512, FPN FRCN 이 있으며 그 중 가장 성능이 좋은 것은 Light Head R-CNN이다.)현재까지도 계속해서 정확도와 성능향상을 위한 많은 연구개발이 진행 중이다. \n",
        "\n",
        "객체 검출 알고리즘은 2가지를 수행해야 된다. 우선 영상에서는 **객체의 위치를 찾아야하고, 그 위치의 객체가 어떤 물체인지를 분류해야 된다.** 즉, Localization과 Classification을 수행하여야한다. 이를 수행하기 위하여 다양한 방법들의 시도되었고, 근래에 2가지 방법으로 좁혀졌다.\n",
        "\n",
        "객체 검출 알고리즘은 단일 단계 검출 방법(Single-stage detector)과 두 단계 검출 방법(Two-stage detector)이 있다. 단일 단계 검출 방법은 모든 영역에서 Localization과 Classification을 동시에 수행한다. 두 단계 검출 방법은 대략적인 Localization을 수행하고, 선출된 후보군에서 분류와 세밀한 Localzation을 수행한다. 일반적으로 단일 단계 검출기가 속도는 더 빠르지만, 정확도는 두 단계 검출기보다 조금 떨어진다. 반대로 두 단계 검출기가 속도는 조금 느리지만, 정확도는 단일 단계 검출기보다 좋다.\n",
        "\n",
        ">단일 단계 검출기(Single-stage detector)    \n",
        "단일 단계 검출기는 앞에서 언급한 것과 같이 모든 영역에서 객체의 위치 검출과 분류를 동시에 수행한다. 이를 동시에 수행하다보니, 속도는 빠르지만, 정확도는 조금 떨어진다. 대표적인 알고리즘으로 YOLO, SSD 등이 있다. 단일 단계 검출기에서 가장 많이 사용되는 SSD의 알고리즘을 살펴보면 처음 RGB 채널의 이미지가 입력으로 들어가게된다. 이후 Deep Nerual Network(DNN) 연산을 수행하여 특징 피라미드(Feature Pyramid)를 만들게 된다. 이 특징 피라미드는 각 객체의 크기에 따른 특징 정보를 가지고 있고, 피라미드에서의 38, 19, 10, 5, 3, 1의 숫자는 Grid를 나타낸다. 즉 빨간 부분에서는 38*38로 나누어 각 부분에서 객체가 있는지를 살펴보게 된다. 빨간 부분은 이미지 내에서 작은 객체에 대한 정보를 가지고 있고, 보라색 부분은 이미지내에서 가장 큰 객체에 대한 정보를 가지고 있다.\n",
        "\n",
        ">>특징 피라미드에서 객체의 위치검출과 분류를 하는 방법에 대해 설명하겠습니다. 모든 영역의 특징 피라미드 내에서 다양한 비율의 객체에 대해서 Localization을 수행하고, 모든 객체의 종류에 따른 분류를 수행한다. 이 알고리즘은 GPU가 포함된 PC환경에서 58fps(Frame per second)의 속도와 25.1의 mAP를 가진다.(MS-COCO 데이터셋 테스트 결과로, 실험 데이터가 매우 어려운 데이터 셋이며, 결과 이미지들을 보면 매우 좋은 성능을 보임)이후 YOLO 또는 SSD와 같은 단일 단계 검출 방법의 성능을 향상시키는 다양한 연구들이 진행되었고, 현재도 계속 진행 중이다.\n",
        "***\n",
        ">두 단계 검출기(Two-stage detector)    \n",
        "두 단계 검출기는 대략적인 객체의 위치를 검출한 후, 검출된 후보군의 분류와 세밀한 객체의 위치 검출을 수행한다. 대표적인 알고리즘으로 Faster-RCNN, Light-Header R-CNN 등 이 있다. 두 단계 검출기에서 가장 많이 사용되는 Faster R-CNN의 알고리즘을 살펴보면 처음 RGB 채널의 이미지가 들어가게 되고, DNN 연산을 수행하여 객체가 있을 만한 위치에 대한 후보군(Region of Interest)들을 찾아낸다(대략적인 Localization), 이후, 후보군들 각각에 대하여 Classification을 수행하고, 후보군 내에서 더욱 정확한 위치에 대한 연산을 수행한다. \n",
        "\n",
        ">>두 단계 검출기는 최근까지도 성능은 뛰어나지만 속도가 오래 걸린다는 단점을 가지고 있었다. MS-COCO 데이터셋 테스트 결과를 살펴보면, Faster-RCNN 또는 R-FCN의 경우 15fps도 되지 않는 것을 볼 수 있다. 하지만, 최근 발표된 Light Head R-CNN이라는 논문은 최근 매우 뛰어난 정확도와 속도를 나타냄을 보여주었다. 현재까지 나온 객체 검출 알고리즘도 매우 성능이 좋고 빠른 알고리즘이 많지만, 현실에 적용하기 위해서는 아직 부족한 부분이 많다. 정확도 측면에서 살펴본다면, 정확도를 나타내는 mAP의 최고 값은 1이다. 하지만, 현재 MS-COCO 데이터 셋에서의 mAP는 20~30 정도의 수준으로 더욱 높은 성능을 요구한다. 속도적인 측면에서 살펴보면, GPU가 포함된 PC환경에서의 속도가 100fps이나 실제 임베디드에 적용하기 위해서는 더욱 빠른 알고리즘을 필요로 한다. 아직까지도 객체 검출 알고리즘을 실제로 적용하기 위해서는 많은 기술 연구 및 개발을 필요로 하고, 계속 연구들이 진행되고 있다.\n",
        "\n",
        "[참고자료](http://global-autonews.com/bbs/board.php?bo_table=bd_035&wr_id=392&page=2)\n",
        "\n",
        "\n",
        "3. 딥러닝과 영상 분석\n",
        "딥러닝은 국소 최저값이라는 문제를 해결하기 위해 데이터의 전처리 과정을 도입해 비지도 학습법(unsupervised learning)으로 각 층을 손질한 다음에 처리한 데이터를 여러 층으로 쌓아올려 최적화를 수행한다. 이와 같은 결과로 인해 여러 신경망 단계를 거쳐 복잡한 함수를 회귀(regression)/분류(classification)할 수 있게 됨으로써, 사람에 필적하는 융통성을 발휘하는 수준에 이르렀다. \n",
        "\n",
        "딥러닝 역시 기계 학습의 범주이므로 최종 목표는 데이터를 사용하여 학습하고 학습 결과로 만들어진 모델을 사용하여 새로운 데이터의 특질(feature)을 예측하는데 있다. 여기서 기계 학습 결과 모델이 예측을 제대로 해내기 위해 특질을 자동으로 학습하는 능력이 필요하다.\n",
        "분류나 회귀 과정에서 사용하기 위해 자동으로 특질을 추출하려면 공통 패턴을 찾아내는 자동화된 알고리즘이 중요하다. 딥러닝에서 합성곱(convolutional)층은 복잡도가 증가(모서리나 얼룩 ➞ 눈/코/입 ➞ 얼굴)하는 비선형 특질의 계층을 이루기 위해 이미지에서 찾아낸 좋은 특질을 다음 층으로 넘기는 과정에 적합하다. 마지막 층은 분류나 회귀를 위해 이 모든 일반화 된 특질을 활용한다. 합성곱 신경망을 사용할 경우, 비선형 특질의 여러 층을 추출해 분류기에 넘겨 이 모든 특질을 합쳐 예측을 해낸다. 이때 층이 너무 얕을 경우 복잡한 특질을 학습할 수 없으므로 아주 깊은 비선형 특질 계층을 차곡차곡 쌓아야 한다. 강력한 계산 능력을 제공하는 고성능 (GP)GPU와 수많은 학습 데이터가 필요한 이유가 바로 여기에 있다.\n",
        "\n",
        "현재 가장 널리 쓰이는 영상 분석 기술인 CNN(Convoultional Neural Network)은 영상 분석을 위한 주류로 자리 잡고 있다. CNN은 유용한 정보를 얻기 위해 입력을 필터링하기 위해 합성곱 계층을 사용하는데, 합성곱은 특질 맵(입력 데이터)와 합성 커널의 각 원을 곱한 다음 합계를 내어 새로운 특질 맵을 생성하는 수학적인 연산이다. 합성곱 계층은 학습이 가능한 매개변수를 포함하므로 형태, 윤곽선, 색상과 같은 유용한 정보를 추출하게끔 학습 과정에서 합성 커널이 자동으로 조정된다. (그림1 은 손글씨를 인식하기 위해 르쿤이 개발한 LeNet-5라는 CNN 참조 아키텍처를 보여준다.) 여러 계층으로 나누어진 합성곱과 서브샘플링 과정을 거쳐 최종적으로 글씨를 파악하는 구조로 되어 있다.\n",
        "\n",
        "4. 객체 판별을 넘어 검출로\n",
        "\n",
        "CNN은 영상이 담고 있는 특정 개체 하나가 무엇인지 판별해내는 능력에서 사람과 대등한 수준에 이르렀다. 하지만 자율주행차량이 수집하는 영상 정보는 특정 객체 하나가 아니라 여러 차량, 사람, 표지판 등 다양한 위치에 다양한 객체를 포함하고 있으므로 판별해야 할 대상 객체가 어떤 영역에 위치하는지부터 찾아야 한다. 게다가 영상에 포함된 객체들은 서로 중첩되거나 배경에 묻혀버리기리도 한다. 전통적인 방법을 사용하여 일단 객체가 있을만한 위치를 파악한 다음에 판별 과정으로 넘어가도 되지만, 객체가 있을만한 위치를 파악하는 작업자체가 이미 객체를 판별하는 과정을 어느 정도 포함하고 있기 때문에 검출 과정에서 효율성과 정확성에 제약이 오기 마련이다.\n",
        "\n",
        "초기에는 R-CNN(Region with CNN)이라는 기법이 등장했다. R-CNN에서는 전처리 과정으로 원하는 영상에 선택적 탐색을 적용하여 객체가 있으리라 추정하는 후보 영역(Region Proposal : RP)을 찾아낸 다음에 각 후보 영역마다 CNN을 적용하는 방법을 사용한다. 후처리 과정으로 전통적인 SVM(Support Vector Machine) 기법을 활용하여 CNN에서 얻은 결과물을 판별한다. 하지만 전반적인 검출 과정에서 RP마다 CNN을 적용해야 하며 RP 자체도 계산이 필요하므로 성능 문제가 발생한다.\n",
        "\n",
        "이와 같은 R-CNN의 문제를 해소하기 위해 Fast R-CNN이라는 개선된 기법이 뒤를 이었다. Fast R-CNN에서는 ROI(Region of Interest) 풀링(pooling, 계산량을 줄이고 과적합(overlifting)을 방지하기 위해 특정영역에서 최돼값이나 최소값을 뽑아내는 통합과정)이라는 층을 도입하여 CNN 특질 맵의 일부 영역에서 정규화된 특징을 추출한다. 따라서 RP계산은 물론이고 RP마다 반복적인 CNN 적용도 필요하지 않으므로 성능을 높일 수 있다. 또한 R-CNN과는 달리 Fast R-CNN은 영상 판별을 위해 SVM을 사용하지 않고 단일 네트워크 내부에 소프트맥스(softmax, 입력 과정에서 받은 값을 정규화시켜 0과1사이의 출력값을 산출해내는 함수이며, 모든 출력값의 총합은 1이 되는 특성이 존재한다.) 층을 배치하는 방법으로 모델 단순화할 수 있다.\n",
        "\n",
        "하지만 Fast R-CNN에도 병목 지점이 존재한다. 바로 후보 영역(RP)을 찾는 부분이며, 선택적 탐색비용이 비싸기 때문에 어떻게든 이를 해결해야 성능을 개선할 수 있다. Faster R-CNN은 독자적인 선택적 탐색 알고리즘을 대신하여 후보 영역 제안을 위해 동일한 CNN 결과를 재사용하는 현명한 방식을 택한다. 즉, 후보 영역 검출과 객체 판별 목적으로 CNN 하나만 훈련하면 된다.\n",
        "\n",
        "5. 데이터, 학습, 시뮬레이션\n",
        "제대로 된 학습 모델을 생성하려먼 학습이 가능하도록 만들어진 양질의 풍부한 데이터가 필요하다. 다양한 센서로부터 수집된 데이터를 정제한 다음에 객체 위치를 표현하는 바운딩 박스와 함께 객체 유형과 의미를 부여하는 레이블링 작업을 거쳐 딥러닝 기법으로 학습을 진행한다.\n",
        "\n",
        "이처럼 학습에 필요한 데이터가 점점 더 중요해짐에 따라 자율주행차량 제조사들은 다양한 방법으로 데이터를 수집하고 있다. 웨이모나 우버의 경우 독자적으로 카메라와 라이다가 장착된 차량을 운행하면서 고해상도 데이터를 수집하고 있으며, 자율주행차량을 위한 영상처리 부품업체로 유명한 모빌아이의 경우 REM(Road Expereince Management)라는 프로그램을 운영하여 대중들로부터 데이터를 수집하기도 한다. 대학교를 비롯한 연구기관에서도 자율주행 차량 학습을 위한 여러 데이터를 제공하고 있다. 캘리포니아 공과대학교에서는 도심 환경의 일반 주행 과정에서 얻은 비디오 영상을 가공해 보행자 레이블링 데이터를 제공하고 있으며, 버클리 공과대학에서는 <Berkeley DeepDrive>라는 사이트를 만들어 다양한 기상, 시각, 상황을 반영한 도심 운행 영상 데이터와 도로 위에 존재하는 객체 레이블링 데이터를 제공하고 있다.\n",
        "\n",
        "앞서 소개한 CNN 계열의 학습 알고리즘을 사용하여 방대한 데이터를 모델로 변환하는 작업을 거치게 된다. 학습시간을 단축하기 위해 대규모 GPU클러스터를 활용하여 병렬로 학습을 수행하게 되며, 이렇게 만들어진 모델을 시뮬레이터에서 돌려 강화학습을 진행하면 점점 더 개선된 결과를 얻게 된다. 웨이모는 일찌감치 카크래프트라는 가상현실을 접목한 시뮬레이터로 자율주행차량의 모델을 개선하는 작업을 벌이고 있으며, 바이두는 자율주행차량 개발에 도움이 되는 아폴로라는 시뮬레이터를 마이크로소프트 에저(Azure) 클라우드 상에서 서비스 형태로 제공하고 있으며, 트루비전은 자율주행을 위한 시뮬레이터를 오픈소스로 공개했다.\n",
        "\n",
        "[PDF파일 검색링크 첨부](https://www.google.com/search?rlz=1C1VSNE_enKR728KR728&sxsrf=ALeKk02gq3vkZJ6Xmt87TrL8FbVGV3XxLA%3A1591700245097&ei=FWvfXoy-BYeJoASR4KjABQ&q=%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89%EC%B0%A8%EB%9F%89%EC%9D%98+%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5+%EA%B8%B0%EC%88%A0+%EB%8F%99%ED%96%A5&oq=%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89%EC%B0%A8%EB%9F%89%EC%9D%98+%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5+%EA%B8%B0%EC%88%A0+%EB%8F%99%ED%96%A5&gs_lcp=CgZwc3ktYWIQAzIECAAQHjoECCMQJ1DJmQRY_5sEYKKfBGgAcAB4AIAB_gGIAdEEkgEFMC4xLjKYAQCgAQGqAQdnd3Mtd2l6&sclient=psy-ab&ved=0ahUKEwjMnMq9yfTpAhWHBIgKHREwClgQ4dUDCAw&uact=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZz3y7d6BGkB",
        "colab_type": "text"
      },
      "source": [
        "[링크 텍스트](https://)"
      ]
    }
  ]
}